#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TensorBoard + PyTorch add_graph è¯Šæ–­è„šæœ¬
å¯ç‹¬ç«‹è¿è¡Œï¼Œç”¨äºæ’æŸ¥ add_graph é™é»˜é€€å‡º / CUDA ä¸å¯ç”¨ / OOM ç­‰é—®é¢˜ã€‚
ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ï¼štensorboard_diagnosis_report.txt
"""

import torch
from torch.utils.tensorboard import SummaryWriter
import os, gc, subprocess, traceback, shutil
from datetime import datetime
from transformer.model import Transformer

REPORT_PATH = "tensorboard_diagnosis_report.txt"

def log(msg, file=None, end="\n"):
    print(msg, end=end)
    if file:
        file.write(msg + "\n")

def section(title, file):
    sep = "=" * 80
    log(f"\n{sep}\nã€{title}ã€‘\n{sep}", file)

def run_cmd(cmd):
    try:
        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, timeout=10)
        return out.decode().strip()
    except Exception as e:
        return f"æ‰§è¡Œå¤±è´¥: {e}"

def check_env_info(f):
    section("PyTorch & ç¯å¢ƒä¿¡æ¯", f)
    log(f"æ—¶é—´: {datetime.now()}", f)
    log(f"PyTorch ç‰ˆæœ¬: {torch.__version__}", f)
    log(f"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}", f)
    log(f"torch.version.cuda: {torch.version.cuda}", f)
    log(f"torch.backends.cudnn.version(): {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A'}", f)

def check_gpu_status(f):
    section("GPU çŠ¶æ€æ£€æµ‹", f)
    if not torch.cuda.is_available():
        log("âš ï¸ CUDA ä¸å¯ç”¨ï¼ˆå½“å‰ä¸º CPU ç¯å¢ƒï¼‰", f)
        log(run_cmd("nvidia-smi"), f)
        return "cpu"

    n = torch.cuda.device_count()
    log(f"âœ… æ£€æµ‹åˆ° {n} ä¸ª GPU", f)
    for i in range(n):
        prop = torch.cuda.get_device_properties(i)
        log(f"GPU[{i}]: {prop.name}, æ˜¾å­˜ {prop.total_memory / 1024**3:.1f} GB", f)
    log(run_cmd("nvidia-smi | head -n 20"), f)
    return "cuda"

def check_dependencies(f):
    section("å…³é”®ä¾èµ–æ£€æŸ¥", f)
    for pkg in ["tensorboard", "protobuf", "onnx"]:
        try:
            __import__(pkg)
            version = run_cmd(f"pip show {pkg} | grep Version") or "æœªçŸ¥ç‰ˆæœ¬"
            log(f"âœ… {pkg} å·²å®‰è£… ({version})", f)
        except ImportError:
            log(f"âŒ {pkg} æœªå®‰è£…", f)

def check_disk_space(f):
    section("ç£ç›˜ä¸æƒé™", f)
    cwd = os.getcwd()
    total, used, free = shutil.disk_usage(cwd)
    log(f"å½“å‰ç›®å½•: {cwd}", f)
    log(f"ç£ç›˜æ€»é‡: {total / 1024**3:.1f} GB, å‰©ä½™: {free / 1024**3:.1f} GB", f)
    test_file = "tensorboard_write_test.txt"
    try:
        with open(test_file, "w") as wf:
            wf.write("test")
        os.remove(test_file)
        log("âœ… å†™å…¥æƒé™æ­£å¸¸", f)
    except Exception as e:
        log(f"âŒ å†™å…¥æƒé™å¼‚å¸¸: {e}", f)

def check_oom_logs(f):
    section("ç³»ç»Ÿ OOM Kill æ£€æŸ¥", f)
    log(run_cmd("dmesg | grep -i 'killed process' | tail -n 5"), f)

def simple_add_graph_test(f, device):
    section("add_graph åŠŸèƒ½æµ‹è¯•", f)    
    # model = SimpleModel().to(device)
    # seq_len = 32
    # dummy_input = torch.randint(0, 10000, (4, seq_len), dtype=torch.long, device=device)
    model = Transformer(
        vocab_size=10000,
        d_model=64,
        seq_len=32,
        n_heads=4,
        d_hidden=256,
        stack=2
    )
    
    writer = SummaryWriter(log_dir="runs/diagnostic_test")
    device = torch.device("cpu")
    try:
        with torch.no_grad():
            writer.add_graph(model, (
                torch.zeros((16, model.seq_len), dtype=torch.long, device=device),
                torch.ones((16, model.seq_len), dtype=torch.long, device=device),
                torch.ones((16, model.seq_len), dtype=torch.bool, device=device),
                torch.ones((16, model.seq_len), dtype=torch.bool, device=device),
                ))
        log("âœ… add_graph æˆåŠŸç”Ÿæˆå›¾æ–‡ä»¶ã€‚", f)
    except RuntimeError as e:
        err = str(e).lower()
        if "out of memory" in err:
            log("âŒ CUDA OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰", f)
        else:
            log("âŒ RuntimeError:", f)
            log(traceback.format_exc(), f)
    except Exception:
        log("âŒ å‘ç”ŸæœªçŸ¥å¼‚å¸¸ï¼š", f)
        log(traceback.format_exc(), f)
    finally:
        writer.close()
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()
        log("ğŸ§¹ æ¸…ç†å®Œæˆã€‚", f)

def generate_report():
    with open(REPORT_PATH, "w") as f:
        check_env_info(f)
        device = check_gpu_status(f)
        check_dependencies(f)
        check_disk_space(f)
        check_oom_logs(f)
        simple_add_graph_test(f, device)

        section("ç»“è®ºä¸å»ºè®®", f)
        if not torch.cuda.is_available():
            log("âš ï¸ æ£€æµ‹åˆ° CUDA ä¸å¯ç”¨ï¼Œå»ºè®®æ£€æŸ¥ï¼š", f)
            log("  1ï¸âƒ£ æ˜¯å¦å®‰è£…äº† GPU ç‰ˆ PyTorch", f)
            log("  2ï¸âƒ£ `nvidia-smi` è¾“å‡ºæ˜¯å¦æ­£å¸¸", f)
            log("  3ï¸âƒ£ ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES æ˜¯å¦è¢«ç¦ç”¨", f)
        else:
            log("âœ… CUDA æ­£å¸¸å¯ç”¨ï¼Œadd_graph æµ‹è¯•é€šè¿‡", f)

        log("\næŠ¥å‘Šå·²ç”Ÿæˆ: " + os.path.abspath(REPORT_PATH), f)

    print(f"\nâœ… å®Œæˆï¼è¯·æŸ¥çœ‹æŠ¥å‘Šæ–‡ä»¶: {REPORT_PATH}")

if __name__ == "__main__":
    generate_report()
